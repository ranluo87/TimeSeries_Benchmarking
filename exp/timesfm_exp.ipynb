{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "451380170da8fe99",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook evaluate the TimesFM model with ambient dataset for benchmarking purpose\n",
    "\n",
    "Github: https://github.com/google-research/timesfm\n",
    "\n",
    "arxiv: https://arxiv.org/abs/2310.10688\n",
    "\n",
    "Frequency definations\n",
    "\n",
    "# 0: T, MIN, H, D, B, U\n",
    "# 1: W, M\n",
    "# 2: Q, Y"
   ]
  },
  {
   "cell_type": "code",
   "id": "ba0c21d1ad990046",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T18:25:46.736032Z",
     "start_time": "2025-04-28T18:25:43.239531Z"
    }
   },
   "source": [
    "from typing import Optional, Tuple\n",
    "from os import path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from dataloader.dataloader import UnivariateMethaneHourly\n",
    "\n",
    "from finetuning.finetuning_torch import FinetuningConfig, TimesFMFinetuner\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "from timesfm import TimesFm, TimesFmCheckpoint, TimesFmHparams\n",
    "from timesfm.pytorch_patched_decoder import PatchedTimeSeriesDecoder\n",
    "import plotly.graph_objects as go\n",
    "import argparse\n",
    "\n",
    "torch.cuda.is_available()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " See https://github.com/google-research/timesfm/blob/master/README.md for updated APIs.\n",
      "Loaded PyTorch TimesFM, likely because python version is 3.11.12 (main, Apr  9 2025, 08:55:54) [GCC 11.4.0].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "cf82faa8-384d-4faf-a322-fb16dcd913ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T18:25:46.742049Z",
     "start_time": "2025-04-28T18:25:46.737457Z"
    }
   },
   "source": [
    "data_dir = '/home/ran/Desktop/PycharmProjects/TimeSeries_Benchmarking/datasets/select'\n",
    "data_file = 'Anzac.csv'\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data_dir', type=str, default=data_dir)\n",
    "parser.add_argument('--data_file', type=str, default=data_file)\n",
    "# TimesFM configurations\n",
    "parser.add_argument('--timesfm', type=bool, default=True)\n",
    "parser.add_argument('--freq_type', type=int, default=0)\n",
    "\n",
    "parser.add_argument('--seq_len', type=int, default=192)\n",
    "parser.add_argument('--pred_len', type=int, default=128)\n",
    "# Optimization Hyperparams\n",
    "parser.add_argument('--batch_size', type=int, default=32)\n",
    "parser.add_argument('--epochs', type=int, default=100)\n",
    "parser.add_argument('--learning_rate', type=float, default=1e-4)\n",
    "\n",
    "args = parser.parse_args('')"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "fcb1df6c295cc65a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T18:25:54.103419Z",
     "start_time": "2025-04-28T18:25:46.744178Z"
    }
   },
   "source": [
    "repo_id = \"google/timesfm-2.0-500m-pytorch\"\n",
    "\n",
    "torch.cuda.set_device(1)\n",
    "hparams = TimesFmHparams(\n",
    "    backend='gpu',\n",
    "    per_core_batch_size=32,\n",
    "    num_layers=50, \n",
    "    horizon_len=args.pred_len,\n",
    "    context_len=args.seq_len,\n",
    "    use_positional_embedding=False,\n",
    "    output_patch_len=128\n",
    ")\n",
    "\n",
    "tfm = TimesFm(\n",
    "    hparams=hparams,\n",
    "    checkpoint=TimesFmCheckpoint(\n",
    "        huggingface_repo_id=repo_id\n",
    "    )\n",
    ")\n",
    "\n",
    "model = PatchedTimeSeriesDecoder(tfm._model_config)\n",
    "\n",
    "# checkpoint_path = path.join(snapshot_download(repo_id), 'torch_model.ckpt')\n",
    "# loaded_checkpoint = torch.load(checkpoint_path, weights_only=True)\n",
    "# model.load_state_dict(loaded_checkpoint)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4bf010d802364be99e4503381cf7ec84"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T18:35:38.136255Z",
     "start_time": "2025-04-28T18:35:36.981411Z"
    }
   },
   "cell_type": "code",
   "source": [
    "config = FinetuningConfig(\n",
    "    batch_size=args.batch_size,\n",
    "    num_epochs=args.epochs,\n",
    "    learning_rate=args.learning_rate,\n",
    "    freq_type=args.freq_type,\n",
    "    log_every_n_steps=10,\n",
    "    val_check_interval=0.2,\n",
    "    use_quantile_loss=True,\n",
    "    use_wandb=False\n",
    ")\n",
    "\n",
    "train_dataset = UnivariateMethaneHourly(args, flag='train')\n",
    "val_dataset = UnivariateMethaneHourly(args, flag='val')\n",
    "model = torch.nn.DataParallel(model, device_ids=[0, 1, 2])\n",
    "\n",
    "finetuner = TimesFMFinetuner(model, config) \n",
    "finetuner.finetune(train_dataset=train_dataset, val_dataset=val_dataset)"
   ],
   "id": "4f6db7d9f5c79def",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51681/51681 [00:00<00:00, 101049.64it/s]\n",
      "100%|██████████| 10824/10824 [00:00<00:00, 209302.36it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "NCCL Error 1: unhandled cuda error (run with NCCL_DEBUG=INFO for details)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 17\u001B[0m\n\u001B[1;32m     14\u001B[0m model \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mDataParallel(model, device_ids\u001B[38;5;241m=\u001B[39m[\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m])\n\u001B[1;32m     16\u001B[0m finetuner \u001B[38;5;241m=\u001B[39m TimesFMFinetuner(model, config) \n\u001B[0;32m---> 17\u001B[0m \u001B[43mfinetuner\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfinetune\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_dataset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_dataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_dataset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mval_dataset\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/PycharmProjects/TimeSeries/.venv/lib/python3.11/site-packages/finetuning/finetuning_torch.py:367\u001B[0m, in \u001B[0;36mTimesFMFinetuner.finetune\u001B[0;34m(self, train_dataset, val_dataset)\u001B[0m\n\u001B[1;32m    365\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    366\u001B[0m   \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mnum_epochs):\n\u001B[0;32m--> 367\u001B[0m     train_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_train_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    368\u001B[0m     val_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate(val_loader)\n\u001B[1;32m    369\u001B[0m     current_lr \u001B[38;5;241m=\u001B[39m optimizer\u001B[38;5;241m.\u001B[39mparam_groups[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlr\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[0;32m~/Desktop/PycharmProjects/TimeSeries/.venv/lib/python3.11/site-packages/finetuning/finetuning_torch.py:295\u001B[0m, in \u001B[0;36mTimesFMFinetuner._train_epoch\u001B[0;34m(self, train_loader, optimizer)\u001B[0m\n\u001B[1;32m    292\u001B[0m num_batches \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(train_loader)\n\u001B[1;32m    294\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch \u001B[38;5;129;01min\u001B[39;00m train_loader:\n\u001B[0;32m--> 295\u001B[0m   loss, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_process_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    297\u001B[0m   optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m    298\u001B[0m   loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[0;32m~/Desktop/PycharmProjects/TimeSeries/.venv/lib/python3.11/site-packages/finetuning/finetuning_torch.py:264\u001B[0m, in \u001B[0;36mTimesFMFinetuner._process_batch\u001B[0;34m(self, batch)\u001B[0m\n\u001B[1;32m    252\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Process a single batch of data.\u001B[39;00m\n\u001B[1;32m    253\u001B[0m \n\u001B[1;32m    254\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    258\u001B[0m \u001B[38;5;124;03m      Tuple of (loss, predictions).\u001B[39;00m\n\u001B[1;32m    259\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m    260\u001B[0m x_context, x_padding, freq, x_future \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    261\u001B[0m     t\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice, non_blocking\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m) \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m batch\n\u001B[1;32m    262\u001B[0m ]\n\u001B[0;32m--> 264\u001B[0m predictions \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_context\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx_padding\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfreq\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    265\u001B[0m predictions_mean \u001B[38;5;241m=\u001B[39m predictions[\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m, \u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    266\u001B[0m last_patch_pred \u001B[38;5;241m=\u001B[39m predictions_mean[:, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, :]\n",
      "File \u001B[0;32m~/Desktop/PycharmProjects/TimeSeries/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1749\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1751\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/PycharmProjects/TimeSeries/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1757\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1758\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1760\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1761\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1762\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1764\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1765\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/Desktop/PycharmProjects/TimeSeries/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:193\u001B[0m, in \u001B[0;36mDataParallel.forward\u001B[0;34m(self, *inputs, **kwargs)\u001B[0m\n\u001B[1;32m    191\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice_ids) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m    192\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodule(\u001B[38;5;241m*\u001B[39minputs[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodule_kwargs[\u001B[38;5;241m0\u001B[39m])\n\u001B[0;32m--> 193\u001B[0m replicas \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreplicate\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodule\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice_ids\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    194\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparallel_apply(replicas, inputs, module_kwargs)\n\u001B[1;32m    195\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgather(outputs, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput_device)\n",
      "File \u001B[0;32m~/Desktop/PycharmProjects/TimeSeries/.venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:200\u001B[0m, in \u001B[0;36mDataParallel.replicate\u001B[0;34m(self, module, device_ids)\u001B[0m\n\u001B[1;32m    197\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mreplicate\u001B[39m(\n\u001B[1;32m    198\u001B[0m     \u001B[38;5;28mself\u001B[39m, module: T, device_ids: Sequence[Union[\u001B[38;5;28mint\u001B[39m, torch\u001B[38;5;241m.\u001B[39mdevice]]\n\u001B[1;32m    199\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mlist\u001B[39m[T]:\n\u001B[0;32m--> 200\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mreplicate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodule\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_grad_enabled\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/PycharmProjects/TimeSeries/.venv/lib/python3.11/site-packages/torch/nn/parallel/replicate.py:126\u001B[0m, in \u001B[0;36mreplicate\u001B[0;34m(network, devices, detach)\u001B[0m\n\u001B[1;32m    124\u001B[0m params \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(network\u001B[38;5;241m.\u001B[39mparameters())\n\u001B[1;32m    125\u001B[0m param_indices \u001B[38;5;241m=\u001B[39m {param: idx \u001B[38;5;28;01mfor\u001B[39;00m idx, param \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(params)}\n\u001B[0;32m--> 126\u001B[0m param_copies \u001B[38;5;241m=\u001B[39m \u001B[43m_broadcast_coalesced_reshape\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevices\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdetach\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    128\u001B[0m buffers \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(network\u001B[38;5;241m.\u001B[39mbuffers())\n\u001B[1;32m    129\u001B[0m buffers_rg: \u001B[38;5;28mlist\u001B[39m[torch\u001B[38;5;241m.\u001B[39mTensor] \u001B[38;5;241m=\u001B[39m []\n",
      "File \u001B[0;32m~/Desktop/PycharmProjects/TimeSeries/.venv/lib/python3.11/site-packages/torch/nn/parallel/replicate.py:95\u001B[0m, in \u001B[0;36m_broadcast_coalesced_reshape\u001B[0;34m(tensors, devices, detach)\u001B[0m\n\u001B[1;32m     92\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     93\u001B[0m     \u001B[38;5;66;03m# Use the autograd function to broadcast if not detach\u001B[39;00m\n\u001B[1;32m     94\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(tensors) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m---> 95\u001B[0m         tensor_copies \u001B[38;5;241m=\u001B[39m \u001B[43mBroadcast\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevices\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mtensors\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     96\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m [\n\u001B[1;32m     97\u001B[0m             tensor_copies[i : i \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mlen\u001B[39m(tensors)]\n\u001B[1;32m     98\u001B[0m             \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;28mlen\u001B[39m(tensor_copies), \u001B[38;5;28mlen\u001B[39m(tensors))\n\u001B[1;32m     99\u001B[0m         ]\n\u001B[1;32m    100\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/Desktop/PycharmProjects/TimeSeries/.venv/lib/python3.11/site-packages/torch/autograd/function.py:575\u001B[0m, in \u001B[0;36mFunction.apply\u001B[0;34m(cls, *args, **kwargs)\u001B[0m\n\u001B[1;32m    572\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39m_are_functorch_transforms_active():\n\u001B[1;32m    573\u001B[0m     \u001B[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001B[39;00m\n\u001B[1;32m    574\u001B[0m     args \u001B[38;5;241m=\u001B[39m _functorch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39munwrap_dead_wrappers(args)\n\u001B[0;32m--> 575\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m    577\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_setup_ctx_defined:\n\u001B[1;32m    578\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    579\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIn order to use an autograd.Function with functorch transforms \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    580\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    581\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstaticmethod. For more details, please see \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    582\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    583\u001B[0m     )\n",
      "File \u001B[0;32m~/Desktop/PycharmProjects/TimeSeries/.venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:23\u001B[0m, in \u001B[0;36mBroadcast.forward\u001B[0;34m(ctx, target_gpus, *inputs)\u001B[0m\n\u001B[1;32m     21\u001B[0m ctx\u001B[38;5;241m.\u001B[39mnum_inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(inputs)\n\u001B[1;32m     22\u001B[0m ctx\u001B[38;5;241m.\u001B[39minput_device \u001B[38;5;241m=\u001B[39m inputs[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mget_device()\n\u001B[0;32m---> 23\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mcomm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbroadcast_coalesced\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_gpus\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     24\u001B[0m non_differentiables \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m idx, input_requires_grad \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(ctx\u001B[38;5;241m.\u001B[39mneeds_input_grad[\u001B[38;5;241m1\u001B[39m:]):\n",
      "File \u001B[0;32m~/Desktop/PycharmProjects/TimeSeries/.venv/lib/python3.11/site-packages/torch/nn/parallel/comm.py:66\u001B[0m, in \u001B[0;36mbroadcast_coalesced\u001B[0;34m(tensors, devices, buffer_size)\u001B[0m\n\u001B[1;32m     64\u001B[0m devices \u001B[38;5;241m=\u001B[39m [_get_device_index(d) \u001B[38;5;28;01mfor\u001B[39;00m d \u001B[38;5;129;01min\u001B[39;00m devices]\n\u001B[1;32m     65\u001B[0m tensors \u001B[38;5;241m=\u001B[39m [_handle_complex(t) \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m tensors]\n\u001B[0;32m---> 66\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_C\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_broadcast_coalesced\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevices\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuffer_size\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: NCCL Error 1: unhandled cuda error (run with NCCL_DEBUG=INFO for details)"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "b34a494c-e886-4cfd-8295-be5230a7f14e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T18:25:58.757019Z",
     "start_time": "2025-04-28T18:25:58.756599Z"
    }
   },
   "source": [
    "raw_df = pd.read_csv(str(path.join(data_dir, data_file)), parse_dates=True)\n",
    "raw_df.reset_index(inplace=True)\n",
    "\n",
    "test_df = raw_df[int(len(raw_df) * 0.8):]\n",
    "test_df.columns = ['unique_id', 'ds', 'values']\n",
    "\n",
    "test_df['ds'] = pd.to_datetime(test_df['ds'])\n",
    "\n",
    "forecast_df = tfm.forecast_on_df(\n",
    "    inputs=test_df,\n",
    "    freq='1H'\n",
    ")\n",
    "\n",
    "forecast_df = forecast_df[['ds', 'timesfm']]\n",
    "forecast_df = forecast_df.groupby(['ds']).mean()\n",
    "forecast_df.reset_index(inplace=True)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(x=forecast_df['ds'], y=forecast_df['timesfm'], mode='lines+markers', name='Forecast'))\n",
    "fig.add_trace(go.Scatter(x=test_df['ds'], y=test_df['values'], mode='lines+markers', name='True'))\n",
    "\n",
    "fig.write_html(\"./timesfm.html\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T18:25:58.759060Z",
     "start_time": "2025-04-28T18:25:58.758704Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "f01cf1cad921c684",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "429ac08f96a943e9",
   "metadata": {},
   "source": [
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
